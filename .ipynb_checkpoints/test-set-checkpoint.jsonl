{"id":"9eb3bc2c-177f-4b27-b1df-4c1591ca54f4","question":"What can I do on Generative AI on AWS?","reference_answer":"You can innovate faster with the most comprehensive set of Generative AI services, browse Generative AI Trainings, get started on generative AI training with content built by AWS experts, and read Generative AI Blogs to get the latest AWS generative AI product news and best practices.","reference_context":"Document 188: Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 169: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 150: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 212: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":188,"topic":"AWS Generative AI Services"}}
{"id":"27dbba10-9ece-4f4f-9268-353c959d15b0","question":"What is the Least-to-most prompting technique in AI?","reference_answer":"In the Least-to-most prompting technique, the model is first prompted to list the subproblems of a problem, and then solve them in sequence. This approach ensures that later subproblems can be solved with the help of answers to previous subproblems.","reference_context":"Document 74: In this technique, the model is prompted to solve the problem, critique its solution, and then resolve the problem considering the problem, solution, and critique. The problem-solving process repeats until a it reaches a predetermined reason to stop. For example, it could run out of tokens or time, or the model could output a stop token.\nFor example, imagine a user prompts a model, \"Write a short essay on literature.\" The model might draft an essay, critique it for lack of specific examples, and rewrite the essay to include specific examples. This process would repeat until the essay is deemed satisfactory or a stop criterion is met.\nDirectional-stimulus prompting\nThis prompt engineering technique includes a hint or cue, such as desired keywords, to guide the language model toward the desired output.\nFor example, if the prompt is to write a poem about love, the prompt engineer may craft prompts that include \"heart,\" \"passion,\" and \"eternal.\" The model might be prompted, \"Write a poem about love that includes the words 'heart,' 'passion,' and 'eternal'.\" This would guide the model to craft a poem with these keywords.\n\nDocument 73: For example, imagine a user prompts the model to write an essay on the effects of deforestation. The model might first generate facts like \"deforestation contributes to climate change\" and \"deforestation leads to loss of biodiversity.\" Then it would elaborate on the points in the essay.\nLeast-to-most prompting\nIn this prompt engineering technique, the model is prompted first to list the subproblems of a problem, and then solve them in sequence. This approach ensures that later subproblems can be solved with the help of answers to previous subproblems.\nFor example, imagine that a user prompts the model with a math problem like \"Solve for x in equation 2x + 3 = 11.\" The model might first list the subproblems as \"Subtract 3 from both sides\" and \"Divide by 2\". It would then solve them in sequence to get the final answer.\nSelf-refine prompting","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":74,"topic":"AI Prompt Engineering"}}
{"id":"0b7deb84-51cd-4f1b-9e5b-ea7fec8f9d12","question":"What programming languages does AWS support for developers?","reference_answer":"AWS supports .NET, Python, Java, PHP, and JavaScript for developers.","reference_context":"Document 80: Getting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nContact Us\nGet Expert Help\nFile a Support Ticket\nAWS re:Post\nKnowledge Center\nAWS Support Overview\nLegal\nAWS Careers\n\n\n\n\n\n\n\n  Create an AWS Account \n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n         Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\nDocument 183: Getting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nContact Us\nGet Expert Help\nFile a Support Ticket\nAWS re:Post\nKnowledge Center\nAWS Support Overview\nLegal\nAWS Careers\n\n\n\n\n\n\n\n  Create an AWS Account \n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n         Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\nDocument 141: Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nContact Us\nGet Expert Help\nFile a Support Ticket\nAWS re:Post\nKnowledge Center\nAWS Support Overview\nLegal\nAWS Careers\n\n\n\n\n\n\n\n  Create an AWS Account \n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n         Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\nDocument 22: Developer Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nContact Us\nGet Expert Help\nFile a Support Ticket\nAWS re:Post\nKnowledge Center\nAWS Support Overview\nLegal\nAWS Careers\n\n\n\n\n\n\n\n  Create an AWS Account \n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n         Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":80,"topic":"Others"}}
{"id":"f296dc47-9420-456f-898c-1563a9330208","question":"What is the price per hour per model with no commitment for Claude Instant in the US East (N. Virginia) and US West (Oregon) regions?","reference_answer":"$44.00","reference_context":"Document 241: Claude Instant \n$0.0008 \n$0.0024 \n\n\nClaude 2.0\/2.1 \n$0.008 \n$0.024 \n\n\nClaude 3 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 3.5 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 3\u00a0Haiku \n$0.00025 \n$0.00125 \n\n\n\n\n\nProvisioned Throughput pricing \n\n\nRegion: US East (N. Virginia) and US West (Oregon) \n\n\n\n\n\nAnthropic models\nPrice per hour per model with  no commitment\nPrice per hour per model unit for 1-month commitment\nPrice per hour per model unit for 6-month commitment\n\n\nClaude Instant \n$44.00\n$39.60 \n$22.00 \n\n\nClaude 2.0\/2.1 \n$70.00\n$63.00 \n$35.00\n\nDocument 237: On-Demand pricing \n\n\nRegion: US East (N. Virginia) and US West (Oregon) \n\n\n\n\n\nAnthropic models\nPrice per 1,000 input tokens\nPrice per 1,000 output tokens\n\n\nClaude 3.5 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 3 Opus* \n$0.015 \n$0.075 \n\n\nClaude 3\u00a0Haiku \n$0.00025 \n$0.00125 \n\n\nClaude 3 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 2.1 \n$0.008 \n$0.024 \n\n\nClaude 2.0 \n$0.008 \n$0.024 \n\n\nClaude Instant \n$0.0008 \n$0.0024 \n\n\n*Claude 3 Opus is currently available in the US West (Oregon) Region\n\n\n\n\n\nRegion: EU (London) \n\n\n\n\n\nAnthropic models\nPrice per 1,000 input tokens\nPrice per 1,000 output tokens\n\nDocument 242: Anthropic models\nPrice per hour per model with  no commitment\nPrice per hour per model unit for 1-month commitment\nPrice per hour per model unit for 6-month commitment\n\n\nClaude Instant \n$44.00\n$39.60 \n$22.00 \n\n\nClaude 2.0\/2.1 \n$70.00\n$63.00 \n$35.00 \n\n\n\n\n\n\n\nRegion: Asia Pacific (Tokyo) \n\n\n\n\n\nAnthropic models\nPrice per hour per model unit for 1-month commitment\nPrice per hour per model unit for 6-month commitment\n\n\nClaude Instant \n$53.00 \n$29.00 \n\n\nClaude 2.0\/2.1 \n$86.00 \n$48.00 \n\n\n\n\n\nRegion: Europe (Frankfurt) \n\n\n\n\n\nAnthropic models\nPrice per hour per model unit for 1-month commitment\nPrice per hour per model unit for 6-month commitment\n\n\nClaude Instant \n$49.00 \n$27.00\n\nDocument 239: Claude 3 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 3\u00a0Haiku \n$0.00025 \n$0.00125 \n\n\n\n\n\nRegion: Asia Pacific (Sydney) \n\n\n\n\n\nAnthropic models\nPrice per 1,000 input tokens\nPrice per 1,000 output tokens\n\n\nClaude 3 Sonnet \n$0.003 \n$0.015 \n\n\nClaude 3\u00a0Haiku \n$0.00025 \n$0.00125 \n\n\n\n\n\nRegion: Asia Pacific (Tokyo) \n\n\n\n\n\nAnthropic models\nPrice per 1,000 input tokens\nPrice per 1,000 output tokens\n\n\nClaude Instant \n$0.0008 \n$0.0024 \n\n\nClaude 2.0\/2.1 \n$0.008 \n$0.024 \n\n\nClaude 3 Haiku \n$0.00025 \n$0.00125 \n\n\nClaude 3.5 Sonnet \n$0.003 \n$0.015","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":241,"topic":"AI Models and Pricing"}}
{"id":"afc367f9-3cfc-47ae-b9e3-e2bf75c3aa94","question":"What is the role of the generator and discriminator in a Generative Adversarial Network (GAN)?","reference_answer":"In a Generative Adversarial Network (GAN), the generator's role is to generate fake data samples by adding random noise. The discriminator's role is to distinguish between the real data and the fake data produced by the generator. During training, the generator continually improves its ability to create realistic data while the discriminator becomes better at telling real from fake. This adversarial process continues until the generator produces data that is so convincing that the discriminator can't differentiate it from real data.","reference_context":"Document 103: Primitive generative models have been used for decades in statistics to aid in numerical data analysis. Neural networks and deep learning were recent precursors for modern generative AI. Variational autoencoders, developed in 2013, were the first deep generative models that could generate realistic images and speech.\nVAEs introduced the capability to create novel variations of multiple data types. This led to the rapid emergence of other generative AI models like generative adversarial networks and diffusion models. These innovations were focused on generating data that increasingly resembled real data despite being artificially created.\nIn 2017, a further shift in AI research occurred with the introduction of transformers. Transformers seamlessly integrated the encoder-and-decoder architecture with an attention mechanism. They streamlined the training process of language models with exceptional efficiency and versatility. Notable models like GPT emerged as foundational models capable of pretraining on extensive corpora of raw text and fine-tuning for diverse tasks.\nTransformers changed what was possible for natural language processing. They empowered generative capabilities for tasks ranging from translation and summarization to answering questions.\n\nDocument 99: Generative adversarial networks\nThe generative adversarial network (GAN) is another generative AI model that builds upon the diffusion model\u2019s concept.\nGANs work by training two neural networks in a competitive manner. The first network, known as the generator, generates fake data samples by adding random noise. The second network called the discriminator, tries to distinguish between real data and the fake data produced by the generator.\u00a0\nDuring training, the generator continually improves its ability to create realistic data while the discriminator becomes better at telling real from fake. This adversarial process continues until the generator produces data that is so convincing that the discriminator can't differentiate it from real data.\nGANs are widely used in generating realistic images, style transfer, and data augmentation tasks.\nVariational autoencoders\nVariational autoencoders (VAEs) learn a compact representation of data called latent space. The latent space is a mathematical representation of the data. You can think of it as a unique code representing the data based on all its attributes. For example, if studying faces, the latent space contains numbers representing eye shape, nose shape, cheekbones, and ears.\n\nDocument 101: Transformer-based models\nThe transformer-based generative AI model builds upon the encoder and decoder concepts of VAEs. Transformer-based models add more layers to the encoder to improve performance on text-based tasks like comprehension, translation, and creative writing.\nTransformer-based models use a self-attention mechanism. They weigh the importance of different parts of an input sequence when processing each element in the sequence.\nAnother key feature is that these AI models implement contextual embeddings. The encoding of a sequence element depends not only on the element itself but also on its context within the sequence.\nHow transformer-based models work\nTo understand how transformer-based models work, imagine a sentence as a sequence of words.\nSelf-attention helps the model focus on the relevant words as it processes each word. The transformer-based generative model employs multiple encoder layers called attention heads to capture different types of relationships between words. Each head learns to attend to different parts of the input sequence, allowing the model to simultaneously consider various aspects of the data.\nEach layer also refines the contextual embeddings, making them more informative and capturing everything from grammar syntax to complex semantic meanings.","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":103,"topic":"GPT and Neural Networks"}}
{"id":"1a18c097-5d6b-44b4-bd1c-b06bb93f24d8","question":"What are some practical applications of conversational AI?","reference_answer":"Some use cases of conversational AI include customer service, personal assistants, and chatbots.","reference_context":"Document 131: How does conversational AI work?\n\nDocument 43: How does artificial intelligence work?\n\nDocument 128: What are some use cases of conversational AI?\n\nDocument 134: What is the difference between conversational AI and generative AI?","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":131,"topic":"Others"}}
{"id":"84ffa293-9a5a-4876-ba2b-ac530da1fe4c","question":"What programming languages does AWS support for developers?","reference_answer":"AWS supports .NET, Python, Java, PHP, and JavaScript for developers.","reference_context":"Document 206: Instant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nDocument 21: Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDocument 163: Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDocument 52: Get started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help\n\nContact Us\nGet Expert Help\nFile a Support Ticket\nAWS re:Post\nKnowledge Center\nAWS Support Overview\nLegal\nAWS Careers\n\n\n\n\n\n\n\n  Create an AWS Account","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":206,"topic":"AWS and Artificial Intelligence"}}
{"id":"785f76df-cbe3-4720-8f47-e5ac12654a0b","question":"What can I do to innovate faster with Generative AI on AWS?","reference_answer":"You can use the most comprehensive set of Generative AI services offered by AWS to innovate faster. You can also get started on generative AI training with content built by AWS experts, and stay updated with the latest AWS generative AI product news and best practices by reading their blogs.","reference_context":"Document 169: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 150: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 212: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 188: Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":169,"topic":"AWS Generative AI Services"}}
{"id":"656e48db-4716-4293-8258-c09cae42ca67","question":"What are some of the features available in the AWS Management Console?","reference_answer":"The AWS Management Console allows you to manage your profile, sign out of AWS Builder ID, access your account settings, manage your billing & cost, manage your security credentials, and access the AWS Personal Health Dashboard.","reference_context":"Document 7: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us \n\n\n\n\n\n\n\n\n\n\n\n\n Products \n Solutions \n Pricing \n Introduction to AWS \n Getting Started \n Documentation \n Training and Certification \n Developer Center \n Customer Success \n Partner Network \n AWS Marketplace \n Support \n AWS re:Post \n Log into Console \n Download the Mobile App\n\nDocument 56: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us \n\n\n\n\n\n\n\n\n\n\n\n\n Products \n Solutions \n Pricing \n Introduction to AWS \n Getting Started \n Documentation \n Training and Certification \n Developer Center \n Customer Success \n Partner Network \n AWS Marketplace \n Support \n AWS re:Post \n Log into Console \n Download the Mobile App\n\nDocument 83: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us \n\n\n\n\n\n\n\n\n\n\n\n\n Products \n Solutions \n Pricing \n Introduction to AWS \n Getting Started \n Documentation \n Training and Certification \n Developer Center \n Customer Success \n Partner Network \n AWS Marketplace \n Support \n AWS re:Post \n Log into Console \n Download the Mobile App\n\nDocument 122: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us \n\n\n\n\n\n\n\n\n\n\n\n\n Products \n Solutions \n Pricing \n Introduction to AWS \n Getting Started \n Documentation \n Training and Certification \n Developer Center \n Customer Success \n Partner Network \n AWS Marketplace \n Support \n AWS re:Post \n Log into Console \n Download the Mobile App","conversation_history":[],"metadata":{"question_type":"simple","seed_document_id":83,"topic":"Amazon Web Services"}}
{"id":"2e52a124-34e5-4318-9eb7-0b496bf127fd","question":"Could you provide examples of how GPT models have been implemented across different sectors to enhance their operations?","reference_answer":"GPT models can be used to analyze customer feedback and summarize it in easily understandable text, enable virtual characters to converse naturally with human players in virtual reality, and provide a better search experience for help desk personnel by querying the product knowledge base with conversational language to retrieve relevant product information.","reference_context":"Document 223: What are examples of some applications that use GPT? \n\nSince its launch, the GPT models have brought artificial intelligence (AI) to numerous applications in various industries. Here are some examples:\n\nGPT models can be used to analyze customer feedback and summarize it in easily understandable text. First, you can collect customer sentiment data from sources like surveys, reviews, and live chats, then you can ask a GPT model to summarize the data.\nGPT models can be used to enable virtual characters to converse naturally with human players in virtual reality.\nGPT models can be used to provide a better search experience for help desk personnel. They can query the product knowledge base with conversational language to retrieve relevant product information.\n\n\n\n\n\n\n\n\nHow can AWS help you run large language models like GPT-3?\n\nDocument 213: What is GPT?\nWhy is GPT important?\nWhat are the use cases of GPT?\nHow does GPT work?\nHow was GPT-3 trained?\nWhat are examples of some applications that use GPT? \nHow can AWS help you run large language models like GPT-3?\n\n\n\n\n\n\n\nWhat is GPT?\n\nGenerative Pre-trained Transformers, commonly known as GPT, are a family of neural network models that uses the transformer architecture and is a key advancement in artificial intelligence (AI) powering generative AI applications such as ChatGPT. GPT models give applications the ability to create human-like text and content (images, music, and more), and answer questions in a conversational manner. Organizations across industries are using GPT models and generative AI for Q&A bots, text summarization, content generation, and search.\n\n\n\n\n\n\n\nWhy is GPT important?\n\nDocument 216: The GPT model can help business analysts efficiently compile large volumes of data. The language models search for the required data and calculate and display the results in a data table or spreadsheet. Some applications can plot the results on a chart or create comprehensive reports.\u00a0\nProduce learning materials\nEducators can use GPT-based software to generate learning materials such as quizzes and tutorials. Similarly, they can use GPT models to evaluate the answers.\nBuild interactive voice assistants\nThe GPT models allow you to build intelligent interactive voice assistants. While many chatbots only respond to basic verbal prompts, the GPT models can produce chatbots with conversational AI capabilities. In addition, these chatbots can converse verbally like humans when paired with other AI technologies.\n\nDocument 214: The GPT models, and in particular, the transformer architecture that they use, represent a significant AI research breakthrough. The rise of GPT models is an inflection point in the widespread adoption of ML because the technology can be used now to automate and improve a wide set of tasks ranging from language translation and document summarization to writing blog posts, building websites, designing visuals, making animations, writing code, ]researching complex topics, and even composing poems. The value of these models lies in their speed and the scale at which they can operate. For example, where you might need several hours to research, write, and edit an article on nuclear physics, a GPT model can produce one in seconds. GPT models have sparked the research in AI towards achieving artificial general intelligence, which means machines can help organizations reach new levels of productivity and reinvent their applications and customer experiences.\n\n\n\n\n\n\n\nWhat are the use cases of GPT?","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":223,"topic":"GPT and Neural Networks"}}
{"id":"76428f52-24be-4157-9d79-b9568492da13","question":"Could you explain the key differences between traditional machine learning models that focus on classifying data points, and generative models which predict features given a certain label, using an example for better understanding?","reference_answer":"Traditional machine learning models are discriminative, focusing on classifying data points and determining the relationship between known and unknown factors. They work by identifying equations that can numerically map unknown and known factors. On the other hand, generative models try to predict features given a certain label, calculating the probability of features and labels occurring together. They learn the distribution of different data features and their relationships, and can recreate new data that were not in the training set.","reference_context":"Document 97: Traditional machine learning models were discriminative or focused on classifying data points. They attempted to determine the relationship between known and unknown factors. For example, they look at images\u2014known data like pixel arrangement, line, color, and shape\u2014and map them to words\u2014the unknown factor. Mathematically, the models worked by identifying equations that could numerically map unknown and known factors as x and y variables.\nGenerative models take this one step further. Instead of predicting a label given some features, they try to predict features given a certain label. Mathematically, generative modeling calculates the probability of x and y occurring together. It learns the distribution of different data features and their relationships.\nFor example, generative models analyze animal images to record variables like different ear shapes, eye shapes, tail features, and skin patterns. They learn features and their relations to understand what different animals look like in general. They can then recreate new animal images that were not in the training set.\nNext, we give some broad categories of generative AI models.\nDiffusion models","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":97,"topic":"Others"}}
{"id":"bc2044e3-77c3-46b4-9ed9-d505d3caea1d","question":"Could you specify some examples of the foundation models that are readily accessible on the Amazon SageMaker JumpStart platform?","reference_answer":"Amazon SageMaker JumpStart provides access to hundreds of foundation models, including top performing publicly available foundation models. New foundation models continue to be added, including Llama 2, Falcon, and Stable Diffusion XL 1.0.","reference_context":"Document 51: Amazon Bedrock offers a choice of high-performing FMs and a broad set of capabilities. You can experiment with various top FMs and privately customize them with your data.\nAmazon SageMaker offers tools to pre-train FMs from scratch so they can be used internally.\nAmazon Elastic Compute Cloud (EC2) Trn1 instances, powered by AWS Trainium chips, are purpose-built for high-performance deep learning (DL) training of generative AI models.\n\nGet started with AI on AWS by creating a free account today!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next Steps with AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Learn more about Artificial Intelligence Services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console\n\nDocument 205: Amazon Bedrock is the easiest way to build and scale generative AI applications with foundation models. Amazon Bedrock is a fully managed service that makes foundation models from Amazon and leading AI startups available through an API, so you can choose from various FMs to find the model that's best suited for your use case.\u00a0With Bedrock, you can speed up developing and deploying scalable, reliable, and secure generative AI applications without managing infrastructure.\nAmazon SageMaker JumpStart, which is a ML hub offering models, algorithms, and solutions, provides access to hundreds of foundation models, including top performing publicly available foundation models. New foundation models continue to be added, including\u00a0Llama 2,\u00a0Falcon, and\u00a0Stable Diffusion XL 1.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next Steps on AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account\n\nDocument 224: Amazon Bedrock\u00a0is the easiest way to build and scale generative AI applications with large language models, also known as foundation models (FMs), similar to GPT-3. Amazon Bedrock gives you access via an API to foundation models from leading AI startups, including AI21 Labs, Anthropic, and Stability AI\u2014along with Amazon\u2019s newest foundation model family, Amazon Titan FMs. With Bedrock\u2019s serverless experience, you can get started quickly, privately customize FMs with your own data, and easily integrate and deploy them into your applications using the AWS tools and capabilities you are familiar with (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage your FMs at scale) without having to manage any infrastructure. Learn more about\u00a0building with foundation models on Amazon Bedrock.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Machine learning next steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nDocument 78: How can AWS support your generative AI requirements?\n\nAmazon Web Services (AWS) offers the breadth and depth of tools to build and use generative AI. For example, you can use these services:\n\nAmazon Q Developer to generate code suggestions ranging from snippets to full functions in real time based on your comments and existing code.\nAmazon Bedrock to accelerate development of generative AI applications using language models through an API, without managing infrastructure.\nAmazon SageMaker JumpStart to discover, explore, and deploy open source language models. For example, you can work with models like OpenLLaMA, RedPajama, MosaicML's MPT-7B, FLAN-T5, GPT-NeoX-20B, and BLOOM.\n\nIf you prefer to create your own models, use Amazon SageMaker. It provides managed infrastructure and tools to accelerate scalable, reliable, and secure model building, training, and deployment.\nGet started with prompt engineering on AWS by creating an account today.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next Steps on AWS","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":51,"topic":"Amazon Generative AI Services"}}
{"id":"11782a57-e90e-403e-addd-ce08199573a0","question":"Could you enumerate the potential issues that might arise in relation to data privacy and security when proprietary data is utilized in the training of generative artificial intelligence models?","reference_answer":"Data privacy and security concerns arise if proprietary data is used to customize generative AI models. Efforts must be made to ensure that the generative AI tools generate responses that limit unauthorized access to proprietary data. Security concerns also arise if there is a lack of accountability and transparency in how AI models make decisions.","reference_context":"Document 106: Despite their advancements, generative AI systems can sometimes produce inaccurate or misleading information. They rely on patterns and data they were trained on and can reflect biases or inaccuracies inherent in that data. Other concerns related to training data include\nSecurity\nData privacy and security concerns arise if proprietary data is used to customize generative AI models. Efforts must be made to ensure that the generative AI tools generate responses that limit unauthorized access to proprietary data. Security concerns also arise if there is a lack of accountability and transparency in how AI models make decisions.\nCreativity\nWhile generative AI can produce creative content, it often lacks true originality. The creativity of AI is bounded by the data it has been trained on, leading to outputs that may feel repetitive or derivative. Human creativity, which involves a deeper understanding and emotional resonance, remains challenging for AI to replicate fully.\nCost\nTraining and running generative AI models require substantial computational resources. Cloud-based generative AI models are more accessible and affordable than trying to build new models from scratch.\nExplainability\n\nDocument 46: Generative AI refers to artificial intelligence systems that can create new content and artifacts such as images, videos, text, and audio from simple text prompts. Unlike past AI, which was limited to analyzing data, generative AI leverages deep learning and massive datasets to produce high-quality, human-like creative outputs. While enabling exciting creative applications, concerns around bias, harmful content, and intellectual property exist. Overall, generative AI represents a major evolution in AI capabilities to generate human language and new content and artifacts in a human-like manner.\nRead about generative AI \u00bb","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":106,"topic":"Artificial Intelligence Applications"}}
{"id":"a0ac008f-f676-4afe-8f0e-b973f177581a","question":"In the context of the Latent Language Model (LLM) process, what is the specific function of the Retrieval-Augmented Generation (RAG) and how does it enhance the responses generated by the LLM?","reference_answer":"With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses.","reference_context":"Document 17: The next question may be\u2014what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics\u2014different data-science approaches to change management can be used.\nThe following diagram shows the conceptual flow of using RAG with LLMs.\n\nDocument 15: Without RAG, the LLM takes the user input and creates a response based on information it was trained on\u2014or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\nCreate external data\nThe new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\nRetrieve relevant information","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":17,"topic":"Retrieval-Augmented Generation in Large Language Models"}}
{"id":"faa82b9f-fec5-452c-853c-6aeec4ae4183","question":"Could you specify the different languages that Amazon's website interface supports for its users?","reference_answer":"Amazon supports the following languages: \u0639\u0631\u0628\u064a, Bahasa Indonesia, Deutsch, English, Espa\u00f1ol, Fran\u00e7ais, Italiano, Portugu\u00eas, Ti\u1ebfng Vi\u1ec7t, T\u00fcrk\u00e7e, \u03a1\u0443\u0441\u0441\u043a\u0438\u0439, \u0e44\u0e17\u0e22, \u65e5\u672c\u8a9e, \ud55c\uad6d\uc5b4, \u4e2d\u6587 (\u7b80\u4f53), \u4e2d\u6587 (\u7e41\u9ad4).","reference_context":"Document 53: Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\n\n\n\n\n\n\n\n\nLanguage\n\u0639\u0631\u0628\u064a\nBahasa Indonesia\nDeutsch\nEnglish\nEspa\u00f1ol\nFran\u00e7ais\nItaliano\nPortugu\u00eas\nTi\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n\n\n\nPrivacy\n|\nAccessibility\n|\nSite Terms\n|\n Cookie Preferences \n|\n\u00a9 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Ending Support for Internet Explorer\n Got it\n\nDocument 119: Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\n\n\n\n\n\n\n\n\nLanguage\n\u0639\u0631\u0628\u064a\nBahasa Indonesia\nDeutsch\nEnglish\nEspa\u00f1ol\nFran\u00e7ais\nItaliano\nPortugu\u00eas\nTi\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n\n\n\nPrivacy\n|\nAccessibility\n|\nSite Terms\n|\n Cookie Preferences \n|\n\u00a9 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Ending Support for Internet Explorer\n Got it\n\nDocument 226: Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\n\n\n\n\n\n\n\n\nLanguage\n\u0639\u0631\u0628\u064a\nBahasa Indonesia\nDeutsch\nEnglish\nEspa\u00f1ol\nFran\u00e7ais\nItaliano\nPortugu\u00eas\nTi\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n\n\n\nPrivacy\n|\nAccessibility\n|\nSite Terms\n|\n Cookie Preferences \n|\n\u00a9 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Ending Support for Internet Explorer\n Got it\n\nDocument 267: Amazon is an Equal Opportunity Employer: \n         Minority \/ Women \/ Disability \/ Veteran \/ Gender Identity \/ Sexual Orientation \/ Age.\n\n\n\n\n\n\n\n\n\nLanguage\n\u0639\u0631\u0628\u064a\nBahasa Indonesia\nDeutsch\nEnglish\nEspa\u00f1ol\nFran\u00e7ais\nItaliano\nPortugu\u00eas\nTi\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n\n\n\nPrivacy\n|\nAccessibility\n|\nSite Terms\n|\n Cookie Preferences \n|\n\u00a9 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Ending Support for Internet Explorer\n Got it","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":119,"topic":"Amazon Web Services Updates"}}
{"id":"763464ef-383b-4b51-8998-c0868b1477ed","question":"What specific advantages does the inclusion of AI technologies bring to the field of Application Performance Monitoring (APM) in terms of predicting and resolving issues?","reference_answer":"AI-based APM tools use historical data to predict issues before they occur. They can also resolve issues in real-time by suggesting effective solutions to your developers. This strategy keeps applications running effectively and addresses bottlenecks.","reference_context":"Document 33: Intelligent document processing (IDP) translates unstructured document formats into usable data. For example, it converts business documents like emails, images, and PDFs into structured information. IDP uses AI technologies like natural language processing (NLP), deep learning, and computer vision to extract, classify, and validate data.\u00a0\nFor example, HM Land Registry (HMLR) handles property titles for over 87 percent of England and Wales. HMLR caseworkers compare and review complex legal documents related to property transactions. The organization deployed an AI application to automate document comparison, cutting review time by 50 percent and supercharging the approval process of property transfer. For more information, read how HMLR uses Amazon Textract.\nRead about IDP \u00bb\nApplication performance monitoring\nApplication performance monitoring (APM) is the process of using software tools and telemetry data to monitor the performance of business-critical applications. AI-based APM tools use historical data to predict issues before they occur. They can also resolve issues in real-time by suggesting effective solutions to your developers. This strategy keeps applications running effectively and addresses bottlenecks.","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":33,"topic":"Artificial Intelligence Applications"}}
{"id":"26c25ad7-425b-444c-9dbe-e01d5bee7395","question":"What are the three primary technological components that contribute to the functionality of Conversational AI and how do they operate?","reference_answer":"The three main technologies used in Conversational AI are Natural language processing (NLP), Natural language understanding (NLU), and Natural language generation (NLG).","reference_context":"Document 132: Conversational AI works using three main technologies.\nNatural language processing\nNatural language processing (NLP) is a set of techniques and algorithms that allow machines to process, analyze, and understand human language. Human language has several features, like sarcasm, metaphors, sentence structure variations, and grammar and usage exceptions. Machine learning (ML) algorithms for NLP allow conversational AI models to continuously learn from vast textual data and recognize diverse linguistic patterns and nuances.\nRead about NLP \u00bb\nNatural language understanding\nNatural language understanding (NLU) is concerned with the comprehension aspect of the system. It ensures that conversational AI models process the language and understand user intent and context. For instance, the same sentence might have different meanings based on the context in which it's used.\nNLU uses machine learning to discern context, differentiate between meanings, and understand human conversation. This is especially crucial when virtual agents have to escalate complex queries to a human agent. NLU makes the transition smooth and based on a precise understanding of the user's need.\nNatural language generation\n\nDocument 45: Natural language processing (NLP) uses neural networks to interpret, understand, and gather meaning from text data. It uses various computing techniques that specialize in decoding and comprehending human language. These techniques allow machines to process words, grammar syntax, and word combinations to process human text and even generate new text. Natural language processing is critical in tasks like summarizing documents, chatbots, and conducting sentiment analysis.\u00a0\nRead about NLP \u00bb\nComputer vision\nComputer vision uses deep learning techniques to extract information and insights from videos and images. Using computer vision, a computer can understand images just like a human would. You can use it to monitor online content for inappropriate images, recognize faces, and classify image details. It is critical in self-driving cars and trucks to monitor the environment and make split-second decisions.\nRead about computer vision \u00bb\nSpeech recognition\nSpeech recognition software uses deep learning models to interpret human speech, identify words, and detect meaning. The neural networks can transcribe speech to text and indicate vocal sentiment. You can use speech recognition in technologies like virtual assistants and call center software to identify meaning and perform related tasks.\nRead about speech-to-text\u00bb\nGenerative AI\n\nDocument 133: After understanding the user's input, the system formulates a coherent and contextually appropriate response. Natural language generation (NLG) enables virtual agents to construct humanlike sentences in a clear, relevant, and linguistically natural manner. NLG uses powerful deep learning algorithms to formulate responses in context. Moreover, as AI chatbots interact more with users and human agents, their responses become refined and more flexible over time.","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":132,"topic":"Artificial Intelligence Applications"}}
{"id":"e874a2f3-e067-40d4-ba0c-d8c90b457abc","question":"What was the exact score that the most recent version of OpenAI's language model, GPT-4, achieved when it was put to the test in the Uniform Bar Examination?","reference_answer":"GPT-4 successfully passed the Uniform Bar Examination with a score of 297 (76%).","reference_context":"Document 199: GPT-2 released in 2019. OpenAI trained it using 1.5 billion parameters (compared to the 117 million parameters used on GPT-1). GPT-3 has a 96-layer neural network and 175 billion parameters and is trained using the 500-billion-word Common Crawl dataset. The popular ChatGPT chatbot is based on GPT-3.5. And GPT-4, the latest version, launched in late 2022 and successfully passed the Uniform Bar Examination with a score of 297 (76%).\nRead more about GPT \u00bb\nAmazon Titan\n\nDocument 174: Open AI's GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output. While we don\u2019t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book.\nAI21 Labs\u2019 Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities.\nCohere\u2019s Command model has similar capabilities and can work in more than 100 different languages.\nLightOn's Paradigm offers foundation models with claimed capabilities that exceed those of GPT-3. All these LLMs come with APIs that allow developers to create unique generative AI applications.\n\nRead more about\u00a0generative AI\u00a0\u00bb\nRead more about\u00a0foundation models\u00a0\u00bb\n\n\n\n\n\n\n\nHow do large language models work?","conversation_history":[],"metadata":{"question_type":"complex","seed_document_id":199,"topic":"GPT and Neural Networks"}}
{"id":"ecd2461d-ce8e-49da-be09-9a9b2860453f","question":"Given that conversational AI has several applications in business and customer interactions, can generative AI be used in a similar manner within the healthcare and life sciences industry?","reference_answer":"One of the most promising use cases of generative AI is to accelerate drug discovery and research. Generative AI uses models to create novel protein sequences with specific properties for designing antibodies, enzymes, vaccines, and gene therapy. Healthcare and life sciences companies can use generative models to design synthetic gene sequences for applications in synthetic biology and metabolic engineering. For example, they can create new biosynthetic pathways or optimize gene expression for biomanufacturing purposes. Lastly, generative AI can be used to create synthetic patient and healthcare data. This is useful to train AI models, simulate clinical trials, or study rare diseases without access to large real-world datasets.","reference_context":"Document 94: Auto companies are using generative AI to deliver better customer service by providing quick responses to the most common customer questions. New material, chip, and part designs can be created with generative AI to optimize manufacturing processes and reduce costs.\nGenerative AI can also be used for synthetic data generation to test applications. This is especially helpful for data not often included in testing datasets (such as defects or edge cases).\nMedia and entertainment\nFrom animations and scripts to full-length movies, generative AI models can produce novel content at a fraction of the cost and time of traditional production.\nHere are other ways you can use generative AI in the industry:\n\nDocument 113: Besides code generation, there are many applications where you can put generative AI to work to achieve a step change in customer experience, employee productivity, business efficiency, and creativity. You can use generative AI to improve customer experience through capabilities such as chatbots, virtual assistants, intelligent contact centers, personalization, and content moderation. You can boost your employees\u2019 productivity with generative AI-powered conversational search, content creation, and text summarization, among others. You can improve business operations with intelligent document processing, maintenance assistants, quality control and visual inspection, and synthetic training data generation. Finally, you can use generative AI to turbocharge the production of all types of creative content, from art and music to text, animation, video, and image generation.\n\nDocument 93: Read more about generative AI for financial services\u00bb\nHealthcare and life sciences\nOne of the most promising use cases of generative AI is to accelerate drug discovery and research. Generative AI uses models to create novel protein sequences with specific properties for designing antibodies, enzymes, vaccines, and gene therapy.\nHealthcare and life sciences companies can use generative models to design synthetic gene sequences for applications in synthetic biology and metabolic engineering. For example, they can create new biosynthetic pathways or optimize gene expression for biomanufacturing purposes.\nLastly, generative AI can be used to create synthetic patient and healthcare data. This is useful to train AI models, simulate clinical trials, or study rare diseases without access to large real-world datasets.\nRead more about generative AI in healthcare and life sciences\u00bb\nAutomotive and manufacturing\nAutomotive companies can use generative AI technology for many purposes, from engineering to in-vehicle experiences and customer service. For instance, they can optimize the design of mechanical parts to reduce drag in vehicle designs or adapt the design of personal assistants.\n\nDocument 87: Generative AI applications like ChatGPT have captured widespread attention and imagination. They can help reinvent most customer experiences and applications, create new applications never seen before, and help customers reach new productivity levels.\nAccording to Goldman Sachs, generative AI could drive a 7 percent (or almost $7 trillion) increase in global gross domestic product (GDP). They also anticipate it could lift productivity growth by 1.5 percentage points over 10 years.\nNext, we give some more benefits of generative AI.\nAccelerates research\nGenerative AI algorithms can explore and analyze complex data in new ways. So, researchers can discover new trends and patterns that may not be otherwise apparent. These algorithms can summarize content, outline multiple solution paths, brainstorm ideas, and create detailed documentation from research notes. This is why generative AI drastically enhances research and innovation.\nFor example, generative AI systems are being used in the pharma industry to generate and optimize protein sequences and significantly accelerate drug discovery.\nEnhances customer experience\nGenerative AI can respond naturally to human conversation and serve as a tool for customer service and personalization of customer workflows.","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":94,"distracting_context":"Conversational AI has several use cases in business processes and customer interactions. We\u2019ve grouped these use cases into four broad categories.\nInformational\nIn an informational context, conversational AI primarily answers customer inquiries or offers guidance on specific topics. For instance, your users can ask customer service chatbots about the weather, product details, or step-by-step recipe instructions. Another example would be AI-driven virtual assistants, which answer user queries with real-time information ranging from world facts to news updates.\nData capture\nYou can use conversational AI tools to collect essential user details or feedback. For instance, you can create more humanlike interactions during an onboarding process. Another scenario would be post-purchase or post-service chats where conversational interfaces gather feedback about the customer journey\u2014experiences, preferences, or areas of dissatisfaction.\nTransactional","topic":"Artificial Intelligence Applications"}}
{"id":"fda29361-7018-44df-9b2d-e8770e814e94","question":"Considering the cost-effectiveness and ability to maintain current information, what are the potential capabilities of foundation models in the context of RAG technology?","reference_answer":"The context does not provide information on what foundation models can do.","reference_context":"Document 197: What are examples of foundation models?\n\nDocument 194: What can foundation models do?","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":197,"distracting_context":"RAG technology brings several benefits to an organization's generative AI efforts.\nCost-effective implementation\nChatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\nCurrent information\nEven if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\nEnhanced user trust","topic":"Large Language Models in Machine Learning"}}
{"id":"06c185cf-1ed7-4ea9-908e-c90c809ac250","question":"Given that AWS's Generative AI services utilize Large Language Models (LLMs) and transformer neural network architecture, what applications can you build, deploy, and run on AWS using these technologies?","reference_answer":"You can build, deploy, and run generative AI applications on AWS.","reference_context":"Document 187: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat are Foundation Models?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS \n            \n\n\n\n\n\n\n\n\n\n\n\n             Check out Generative AI on AWS\n\nDocument 168: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat are Large Language Models (LLM)?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS\n\nDocument 149: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat is Stable Diffusion?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS\n\nDocument 211: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat is GPT?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":187,"distracting_context":"Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\nTransformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\nUnlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\nTransformer neural network architecture allows the use of very large models, often with hundreds of billions of parameters. Such large-scale models can ingest massive amounts of data, often from the internet, but also from sources such as the Common Crawl, which comprises more than 50 billion web pages, and Wikipedia, which has approximately 57 million pages.\nRead more about\u00a0neural networks\u00a0\u00bb","topic":"AWS and Artificial Intelligence"}}
{"id":"694183b9-e2ef-4ee5-984b-5089e973c141","question":"Considering the historical development of Artificial Intelligence, could you provide some examples of AI applications that emerged from the period between 1957 and 1974, and from the 1980s to the early 2000s?","reference_answer":"The context does not provide any examples of Artificial Intelligence.","reference_context":"Document 31: Artificial Intelligence Examples","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":31,"distracting_context":"Artificial intelligence technology has become increasingly popular due to generative AI tools gaining prominence in the public space. However, the technology has been around for several decades now and is continuously maturing. In his seminal paper from 1950, \"Computing Machinery and Intelligence,\" Alan Turing considered whether machines could think. In this paper, Turing first coined the term artificial intelligence and presented it as a theoretical and philosophical concept.\u00a0\nThe past\nBetween 1957 and 1974, developments in computing allowed computers to store more data and process faster. During this period, scientists further developed machine learning (ML) algorithms. The progress in the field led agencies like the Defense Advanced Research Projects Agency (DARPA) to create a fund for AI research. At first, the main goal of this research was to discover whether computers could transcribe and translate spoken language.\nThrough the 1980s, the boosted funding available and the expanding algorithmic toolkit scientists used in AI streamlined development. David Rumelhart and John Hopfield published papers on deep learning techniques, which showed that computers could learn from experience.\u00a0\nFrom 1990 to the early 2000s, scientists achieved many core goals of AI, like beating the reigning world chess champion.\nThe present","topic":"Artificial Intelligence Applications"}}
{"id":"20ac9c1c-991c-4f6f-8c1e-3440de35b787","question":"According to OpenAI, what is the growth rate of computational power needed for foundation modeling, and how could this impact the use of Generative AI services on AWS?","reference_answer":"According to OpenAI, the computational power required for foundation modeling has doubled every 3.4 months since 2012.","reference_context":"Document 174: Open AI's GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output. While we don\u2019t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book.\nAI21 Labs\u2019 Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities.\nCohere\u2019s Command model has similar capabilities and can work in more than 100 different languages.\nLightOn's Paradigm offers foundation models with claimed capabilities that exceed those of GPT-3. All these LLMs come with APIs that allow developers to create unique generative AI applications.\n\nRead more about\u00a0generative AI\u00a0\u00bb\nRead more about\u00a0foundation models\u00a0\u00bb\n\n\n\n\n\n\n\nHow do large language models work?\n\nDocument 191: For example, BERT, one of the first bidirectional foundation models, was released in 2018. It was trained using 340 million parameters and a 16 GB training dataset. In 2023, only five years later, OpenAI trained GPT-4 using 170 trillion parameters and a 45 GB training dataset. According to OpenAI, the computational power required for foundation modeling has doubled\u00a0every 3.4 months since 2012. Today\u2019s FMs, such as the large language models (LLMs) Claude 2 and Llama 2, and the text-to-image model Stable Diffusion from Stability AI, can perform a range of tasks out of the box spanning multiple domains, like writing blog posts, generating images, solving math problems, engaging in dialog, and answering questions based on a document.\nRead about large language models \u00bb\n\nDocument 199: GPT-2 released in 2019. OpenAI trained it using 1.5 billion parameters (compared to the 117 million parameters used on GPT-1). GPT-3 has a 96-layer neural network and 175 billion parameters and is trained using the 500-billion-word Common Crawl dataset. The popular ChatGPT chatbot is based on GPT-3.5. And GPT-4, the latest version, launched in late 2022 and successfully passed the Uniform Bar Examination with a score of 297 (76%).\nRead more about GPT \u00bb\nAmazon Titan","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":174,"distracting_context":"Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices","topic":"Large Language Models in Machine Learning"}}
{"id":"3ca73520-c883-4c85-be32-b17887425226","question":"Considering that GPT models are neural network-based language prediction models built on the Transformer architecture and they analyze natural language queries, what is the significance of testing in the development of such a generative AI system?","reference_answer":"Testing plays a crucial role in the development of a generative AI system. It involves developing both automated and manual testing processes to validate results and test all types of scenarios the AI system may experience. Different groups of beta testers are used to try out the applications in different ways and document the results. Through continuous testing, the model improves, and there is more control over expected outcomes and responses.","reference_context":"Document 111: Test extensively\nDevelop automated and manual testing processes to validate results and test all types of scenarios the generative AI system may experience. Have different groups of beta testers who try out the applications in different ways and document results. The model will also improve continuously through testing, and you get more control over expected outcomes and responses.\n\n\n\n\n\n\n\nWhat are the common applications of generative AI?","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":111,"distracting_context":"Though it\u2019s accurate to describe the GPT models as artificial intelligence (AI), this is a broad description. More specifically, the GPT models are neural network-based language prediction models built on the Transformer architecture. They analyze natural language queries, known as prompts, and predict the best possible response based on their understanding of language.\nTo do that, the GPT models rely on the knowledge they gain after they\u2019re trained with hundreds of billions of parameters on massive language datasets. They can take input context into account and dynamically attend to different parts of the input, making them capable of generating long responses, not just the next word in a sequence. For example, when asked to generate a piece of Shakespeare-inspired content, a GPT model does so by remembering and reconstructing new phrases and entire sentences with a similar literary style.","topic":"Artificial Intelligence Applications"}}
{"id":"875b811e-2075-4cbe-8ff5-b81ca21ea8b6","question":"In the context of Retrieval-Augmented Generation, what are the two principal modules of the transformer neural network architecture?","reference_answer":"The two main modules of the transformer neural network architecture are the encoder and the self-attention mechanism.","reference_context":"Document 101: Transformer-based models\nThe transformer-based generative AI model builds upon the encoder and decoder concepts of VAEs. Transformer-based models add more layers to the encoder to improve performance on text-based tasks like comprehension, translation, and creative writing.\nTransformer-based models use a self-attention mechanism. They weigh the importance of different parts of an input sequence when processing each element in the sequence.\nAnother key feature is that these AI models implement contextual embeddings. The encoding of a sequence element depends not only on the element itself but also on its context within the sequence.\nHow transformer-based models work\nTo understand how transformer-based models work, imagine a sentence as a sequence of words.\nSelf-attention helps the model focus on the relevant words as it processes each word. The transformer-based generative model employs multiple encoder layers called attention heads to capture different types of relationships between words. Each head learns to attend to different parts of the input sequence, allowing the model to simultaneously consider various aspects of the data.\nEach layer also refines the contextual embeddings, making them more informative and capturing everything from grammar syntax to complex semantic meanings.\n\nDocument 219: There are different types of neural networks, like recurrent and convolutional. The GPT models are transformer neural networks. The transformer neural network architecture uses self-attention mechanisms to focus on different parts of the input text during each processing step. A transformer model captures more context and improves performance on natural language processing (NLP) tasks. It has two main modules, which we explain next.\nRead about neural networks \u00bb\nRead about natural language processing (NLP) \u00bb\nEncoder\u00a0\nTransformers pre-process text inputs as embeddings, which are mathematical representations of a word. When encoded in vector space, words that are closer together are expected to be closer in meaning. These embeddings are processed through an encoder component that captures contextual information \u00a0from an input sequence. When it receives input, the transformer network\u2019s encoder block separates words into embeddings and assigns weight to each. Weights are parameters to indicate the relevance of words in a sentence.\nAdditionally, position encoders allow GPT models to prevent ambiguous meanings when a word is used in other parts of a sentence. For example, position encoding allows the transformer model to differentiate the semantic differences between these sentences:\n\nDocument 103: Primitive generative models have been used for decades in statistics to aid in numerical data analysis. Neural networks and deep learning were recent precursors for modern generative AI. Variational autoencoders, developed in 2013, were the first deep generative models that could generate realistic images and speech.\nVAEs introduced the capability to create novel variations of multiple data types. This led to the rapid emergence of other generative AI models like generative adversarial networks and diffusion models. These innovations were focused on generating data that increasingly resembled real data despite being artificially created.\nIn 2017, a further shift in AI research occurred with the introduction of transformers. Transformers seamlessly integrated the encoder-and-decoder architecture with an attention mechanism. They streamlined the training process of language models with exceptional efficiency and versatility. Notable models like GPT emerged as foundational models capable of pretraining on extensive corpora of raw text and fine-tuning for diverse tasks.\nTransformers changed what was possible for natural language processing. They empowered generative capabilities for tasks ranging from translation and summarization to answering questions.","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":101,"distracting_context":"How does Retrieval-Augmented Generation work?","topic":"GPT and Neural Networks"}}
{"id":"0eb9d821-1eee-48cd-b423-80dc173923f7","question":"Considering the context of customer support and personalized responses, what advantages does the application of Retrieval-Augmented Generation bring to AI technology?","reference_answer":"RAG technology brings several benefits to an organization's generative AI efforts. These include cost-effective implementation, as it is a more affordable approach to introducing new data to the LLM. It also ensures current information, allowing developers to provide the latest research, statistics, or news to the generative models. Finally, it enhances user trust, as the LLM uses new knowledge and its training data to create better responses.","reference_context":"Document 10: Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n\n\n\n\n\n\n\nWhy is Retrieval-Augmented Generation important?\n\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\nKnown challenges of LLMs include:\n\nDocument 11: Presenting false information when it does not have the answer.\nPresenting out-of-date or generic information when the user expects a specific, current response.\nCreating a response from non-authoritative sources.\nCreating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n\n\n\n\n\n\n\nWhat are the benefits of Retrieval-Augmented Generation?\n\nDocument 12: RAG technology brings several benefits to an organization's generative AI efforts.\nCost-effective implementation\nChatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\nCurrent information\nEven if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\nEnhanced user trust\n\nDocument 15: Without RAG, the LLM takes the user input and creates a response based on information it was trained on\u2014or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\nCreate external data\nThe new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\nRetrieve relevant information","conversation_history":[],"metadata":{"question_type":"distracting element","seed_document_id":10,"distracting_context":"Conversational artificial intelligence (AI) is a technology that makes software capable of understanding and responding to voice-based or text-based human conversations. Traditionally, human chat with software has been limited to preprogrammed inputs where users enter or speak predetermined commands. Conversational AI goes much beyond that. It can recognize all types of speech and text input, mimic human interactions, and understand and respond to queries in various languages. Organizations use conversational AI for various customer support use cases, so the software responds to customer queries in a personalized manner.\n\n\n\n\n\n\n\nWhat are the benefits of conversational AI?","topic":"Retrieval-Augmented Generation in Large Language Models"}}
{"id":"94d203ec-f681-464a-a362-a5397200ef4e","question":"As a software engineer working on advanced AI application development, I'm curious about how Amazon Bedrock and Amazon Kendra can support Retrieval-Augmented Generation requirements. Can you provide some insights?","reference_answer":"Amazon Bedrock is a fully-managed service that offers high-performing foundation models and a broad set of capabilities to build generative AI applications, simplifying development and maintaining privacy and security. It can connect to your data sources for RAG in a few clicks, with vector conversions, retrievals, and improved output generation handled automatically. Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that can be used with Amazon Kendra\u2019s high-accuracy semantic ranker as an enterprise retriever for RAG workflows.","reference_context":"Document 19: How can AWS support your\u00a0Retrieval-Augmented Generation requirements?\n\nAmazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models\u2014along with a broad set of capabilities\u2014to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically.\nFor organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra\u2019s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can:","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":19,"situational_context":"A software engineer is seeking additional insights on how Amazon Bedrock can meet their advanced AI application development needs.","topic":"Amazon Generative AI Services"}}
{"id":"6d30d984-5a79-473b-a267-98b0a5a2dbcf","question":"Hi, I'm an application developer exploring different models of Amazon Bedrock for text summarization and image generation. I'm trying to understand the different pricing schemes. Can you tell me the total cost I would incur if I make a request to AI21\u2019s Jurassic-2 Mid model to summarize an input of 10K tokens of input text to an output of 2K tokens?","reference_answer":"The total cost incurred is $0.15.","reference_context":"Document 252: An application developer buys two model units of Amazon Titan Text Express with a 1-month commitment for their text summarization use case.\nTotal monthly cost incurred = 2 model units * $18.40\/hour * 24 hours * 31 days = $27,379.20\nAn application developer buys one model unit of the base Amazon Titan Image Generator model with a 1-month commitment.\nTotal cost incurred = 1 model unit * $16.20 * 24 hours * 31 days = $12,052.80\n\n\n \n   Anthropic \n\n\n On-Demand pricing\n\nAn application developer makes the following API calls to Amazon Bedrock in the US West (Oregon) Region: a request to Anthropic\u2019s Claude model to summarize an input of 11K tokens of input text to an output of 4K tokens.\nTotal cost incurred = 11K tokens\/1000 * $0.008 + 4K tokens\/1000 * $0.024 = $0.088 + $0.096 = $0.184 \n\n Provisioned Throughput pricing\n\nDocument 250: An application developer makes the following API calls to Amazon Bedrock: a request to AI21\u2019s Jurassic-2 Mid model to summarize an input of 10K tokens of input text to an output of 2K tokens. \nTotal cost incurred = 10K tokens\/1000 * $0.0125 + 2K tokens\/1000 * $0.0125 = $0.15 \n\n\n \n   Amazon \n\n\n On-Demand pricing\n\nAn application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Amazon Titan Text Lite model to summarize an input of 2K tokens of input text to an output of 1K tokens.\nTotal hourly cost incurred is = 2K tokens\/1000 * $0.0003 + 1K tokens\/1000 * $0.0004 = $0.001.\nAn application developer makes the following API calls to Amazon Bedrock: a request to the Amazon Titan Image Generator base model to generate 1000 images of 1024 x 1024 in size of standard quality.\nTotal cost incurred = 1000 images * $0.01 per image = $10\n\n Customization (fine-tuning and continued pretraining) pricing\n\nDocument 258: An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mistral 7B model to summarize an input of 2K tokens of input text to an output of 1K tokens.\nTotal hourly cost incurred = 2K tokens\/1000 * $0.00015 + 1K tokens\/1000 * $0.0002 = $0.0005\nAn application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mixtral 8x7B model to summarize an input of 2K tokens of input text to an output of 1K tokens.\nTotal hourly cost incurred = 2K tokens\/1000 * $0.00045 + 1K tokens\/1000 * $0.0007 = $0.0016\nAn application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mistral Large model to summarize an input of 2K tokens of input text to an output of 1K tokens.\u00a0\nTotal hourly cost incurred = 2K tokens\/1000 * $0.008 + 1K tokens\/1000 * $0.024 = $0.04\n\nDocument 256: An application developer makes the following API calls to Amazon Bedrock: a request to Meta\u2019s Llama 2 Chat (13B) model to summarize an input of 2K tokens of input text to an output of 500 tokens.\nTotal cost incurred = 2K tokens\/1000 * $0.00075 + 500 tokens\/1000 * $0.001 = $0.002\n\n Customization (fine-tuning) pricing\n\nAn application developer customizes the Llama 2 Pretrained (70B) model using 1000 tokens of data. After training, uses custom model provisioned throughput for 1 hour to evaluate the performance of the model. The fine-tuned model is stored for 1 month. After evaluation, the developer uses provisioned throughput (1mo commit) to host the customized model.\nMonthly cost incurred for fine-tuning = Fine tuning training ($0.00799 * 1000) + custom model storage per month ($1.95) + 1 hour of custom model inference ($23.50) = $33.44\nMonthly cost incurred for provisioned throughput (a 1-month commit) of custom model = $21.18\n\n Provisioned Throughput pricing","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":252,"situational_context":"An application developer is trying to comprehend the cost incurred while using different models of Amazon Bedrock for text summarization and image generation under different pricing schemes.","topic":"AI Models and Pricing"}}
{"id":"64f98b9f-5aeb-4eb4-b228-c02e4bdf0321","question":"As a software student working on a project, I'm trying to understand Amazon's features better. Can you tell me what the functionality of Stable Diffusion is?","reference_answer":"The context does not provide information on what Stable Diffusion can do.","reference_context":"Document 159: What can Stable Diffusion do?\n\nDocument 155: What architecture does Stable Diffusion use?","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":159,"situational_context":"A software student is trying to learn more about Amazon's Stable Diffusion feature for a project.","topic":"Stable Diffusion AI Model"}}
{"id":"76090496-755a-485a-88a8-54d0964f9247","question":"Hey, I'm a little puzzled by something in the Amazon Bedrock User Guide. Could you tell me what the three main technologies are that conversational AI, like this chatbot, works with?","reference_answer":"Conversational AI works with three main technologies: Natural Language Processing (NLP), Natural Language Understanding (NLU), and Natural Language Generation (NLG).","reference_context":"Document 133: After understanding the user's input, the system formulates a coherent and contextually appropriate response. Natural language generation (NLG) enables virtual agents to construct humanlike sentences in a clear, relevant, and linguistically natural manner. NLG uses powerful deep learning algorithms to formulate responses in context. Moreover, as AI chatbots interact more with users and human agents, their responses become refined and more flexible over time.\n\nDocument 132: Conversational AI works using three main technologies.\nNatural language processing\nNatural language processing (NLP) is a set of techniques and algorithms that allow machines to process, analyze, and understand human language. Human language has several features, like sarcasm, metaphors, sentence structure variations, and grammar and usage exceptions. Machine learning (ML) algorithms for NLP allow conversational AI models to continuously learn from vast textual data and recognize diverse linguistic patterns and nuances.\nRead about NLP \u00bb\nNatural language understanding\nNatural language understanding (NLU) is concerned with the comprehension aspect of the system. It ensures that conversational AI models process the language and understand user intent and context. For instance, the same sentence might have different meanings based on the context in which it's used.\nNLU uses machine learning to discern context, differentiate between meanings, and understand human conversation. This is especially crucial when virtual agents have to escalate complex queries to a human agent. NLU makes the transition smooth and based on a precise understanding of the user's need.\nNatural language generation","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":133,"situational_context":"An Amazon Bedrock user is puzzled by an unclear function and seeks clarity.","topic":"Artificial Intelligence Applications"}}
{"id":"b6a878ae-1b53-40a4-9103-f8e9303df12a","question":"Hey, as a developer, I'm trying to find the most cost-effective option for my generative AI application. Could you tell me about the two pricing plans for inference in Amazon Bedrock?","reference_answer":"The two pricing plans for inference in Amazon Bedrock are: 1. On-Demand and Batch, which allows you to use Foundation Models (FMs) on a pay-as-you-go basis without having to make any time-based term commitments. 2. Provisioned Throughput, which allows you to provision sufficient throughput to meet your application's performance requirements in exchange for a time-based term commitment.","reference_context":"Document 231: Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\nWith Amazon Bedrock, you will be charged for model inference and customization. You have a choice of two pricing plans for inference: 1. On-Demand and Batch: This mode allows you to use FMs on a pay-as-you-go basis without having to make any time-based term commitments. 2. Provisioned Throughput: This mode allows you to provision sufficient throughput to meet your application's performance requirements in exchange for a time-based term commitment.\n\n\n\n\n\n\n\n\n\n\n\n Pricing models\n\n\n On-Demand\n\nDocument 4: Amazon Bedrock pricing\nWhen you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon Bedrock. However, you are charged only for the services that you use.\nTo see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost Management console. To learn more about AWS account billing, see the AWS Billing User Guide. If you have\n   questions concerning AWS billing and AWS accounts, contact AWS Support.\nWith Amazon Bedrock, you pay to run inference on any of the third-party foundation models. Pricing\n   is based on the volume of input tokens and output tokens, and on whether you have purchased provisioned\n   throughput for the model. For more information, see the Model providers page in the Amazon Bedrock console. For each model, pricing is listed\n   following the model version. For more information about purchasing Provisioned Throughput, see \n Provisioned Throughput for Amazon Bedrock.\nFor more information, see Amazon Bedrock Pricing.\n\nDocument 205: Amazon Bedrock is the easiest way to build and scale generative AI applications with foundation models. Amazon Bedrock is a fully managed service that makes foundation models from Amazon and leading AI startups available through an API, so you can choose from various FMs to find the model that's best suited for your use case.\u00a0With Bedrock, you can speed up developing and deploying scalable, reliable, and secure generative AI applications without managing infrastructure.\nAmazon SageMaker JumpStart, which is a ML hub offering models, algorithms, and solutions, provides access to hundreds of foundation models, including top performing publicly available foundation models. New foundation models continue to be added, including\u00a0Llama 2,\u00a0Falcon, and\u00a0Stable Diffusion XL 1.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next Steps on AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account\n\nDocument 224: Amazon Bedrock\u00a0is the easiest way to build and scale generative AI applications with large language models, also known as foundation models (FMs), similar to GPT-3. Amazon Bedrock gives you access via an API to foundation models from leading AI startups, including AI21 Labs, Anthropic, and Stability AI\u2014along with Amazon\u2019s newest foundation model family, Amazon Titan FMs. With Bedrock\u2019s serverless experience, you can get started quickly, privately customize FMs with your own data, and easily integrate and deploy them into your applications using the AWS tools and capabilities you are familiar with (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage your FMs at scale) without having to manage any infrastructure. Learn more about\u00a0building with foundation models on Amazon Bedrock.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Machine learning next steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":231,"situational_context":"A developer needs to understand the pricing models of Amazon Bedrock service to choose the most cost-effective option for their generative AI application.","topic":"AI Models and Pricing"}}
{"id":"1eb4549b-08ce-459e-9c71-ed5b013913ac","question":"Hey, I'm about to start an AWS project and I'm trying to educate myself about Amazon Bedrock's pricing structure and commands. Can you tell me what the price per hour per model with no commitment for the Cohere Command model is?","reference_answer":"$49.50","reference_context":"Document 243: Claude 2.0\/2.1 \n$79.00 \n$44.00 \n\n\n\n\n\nPlease reach out to your AWS account team for more details on model units.\u00a0 \n\n\n\n\n\n\n\n Cohere\n\nOn-Demand pricing \n\n\n\n\n\nCohere models\nPrice per 1,000 input tokens\nPrice per 1,000 output tokens\n\n\nCommand\n$0.0015\n$0.0020\n\n\nCommand-Light\n$0.0003\n$0.0006\n\n\nCommand R+\n$0.0030\n$0.0150\n\n\nCommand R\n$0.0005\n$0.0015\n\n\nEmbed - English\n$0.0001\nN\/A\n\n\nEmbed - Multilingual\n$0.0001\nN\/A\n\n\n\n\n\nPricing for customization (fine-tuning) \n\n\n\n\n\nCohere models\nPrice to train 1,000 tokens\nPrice to store each custom model per month\nPrice to infer from a custom model per model unit per hour (with no-commit Provisioned Throughput pricing)\n\nDocument 244: Cohere Command \n$0.004 \n$1.95 \n$49.50 \n\n\nCohere Command-Light\n$0.001\n$1.95 \n$8.56\n\n\n\n\n\n*Total tokens trained = number of tokens in training data corpus x number of epochs \n\n\n\n\nProvisioned Throughput pricing \n\n\n\n\n\nCohere models\nPrice per hour per model\u00a0 with no commitment\nPrice per hour per model unit for 1-month commitment\nPrice per hour per model unit for 6-month commitment \n\n\nCohere Command \n$49.50\n$39.60 \n$23.77 \n\n\nCohere Command - Light\n$8.56\n$6.85 \n$4.11\n\n\nEmbed - English\n$7.12\n$6.76 \n$6.41\n\n\nEmbed - Multilingual\n$7.12\n$6.76 \n$6.41\n\n\n\n\n\nPlease reach out to your AWS account or sales team for more details on model units.\u00a0  \n\n\n\n\n\n\n\n Meta Llama","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":243,"situational_context":"Looking to educate oneself about Amazon Bedrock's pricing structure and commands before starting an AWS project.","topic":"AI Models and Pricing"}}
{"id":"5c0f531e-0362-40c7-8966-ada1f9eed1f4","question":"I'm a writer working on an article about the integration of LLMs in the workplace. Could you help me understand how LLMs are expected to enhance the performance of automated virtual assistants according to the Amazon Bedrock User Guide?","reference_answer":"LLMs will be better able to interpret user intent and respond to sophisticated commands.","reference_context":"Document 180: LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service chatbots, and simple automated copywriting.\nConversational AI\nLLMs will undoubtedly improve the performance of automated virtual assistants like Alexa, Google Assistant, and Siri. They will be better able to interpret user intent and respond to sophisticated commands.","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":180,"situational_context":"A writer is planning to draft an article about the integration of LLMs in workplace and needs information from Amazon Bedrock User Guide.","topic":"Artificial Intelligence Applications"}}
{"id":"fe6cb5af-2493-4e36-985f-58e67d5c0f0f","question":"Hey, as a developer trying to optimize my generative AI application and manage inappropriate content, I'm considering different models. Could you tell me about the benefits of purchasing Provisioned Throughput for a foundation model?","reference_answer":"Purchasing Provisioned Throughput for a foundation model improves your FM-based application's efficiency and output. It allows you to run inference on models more efficiently and at discounted rates.","reference_context":"Document 3: Improve your FM-based application's efficiency and output \u2013 Purchase Provisioned Throughput for a foundation model in order to run inference on models more efficiently and at discounted rates.\n\nDetermine the best model for your use case \u2013 Evaluate outputs of different models with built-in or custom prompt datasets to determine the model that is best suited for your application.\n\nPrevent inappropriate or unwanted content \u2013 Use guardrails to implement safeguards for your generative AI applications.","conversation_history":[],"metadata":{"question_type":"situational","seed_document_id":3,"situational_context":"A developer is looking to optimize his application's efficiency and requires guidance on selecting the best model for his generative AI application and overseeing inappropriate content.","topic":"Others"}}
{"id":"7727c3b5-780b-4fba-826f-47a02d0e853b","question":"What resources and services does AWS offer for generative AI training and innovation?","reference_answer":"AWS offers content built by experts and Generative AI Blogs for training, as well as a comprehensive set of Generative AI services to help innovate faster.","reference_context":"Document 85: Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is generative AI?\nWhy is generative AI important?\nHow does generative AI work?\nHow will generative AI affect industries?\nHow do generative AI models work?\nHow did generative AI technology evolve?\nWhat are the limitations of generative AI?\nWhat are the best practices in generative AI adoption?\nWhat are the common applications of generative AI?\nHow can AWS help Generative AI?\n\n\n\n\n\n\n\nWhat is generative AI?\n\nDocument 169: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 150: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 212: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What resources does AWS offer for generative AI training?","answer":"AWS offers content built by experts for generative AI training. They also provide Generative AI Blogs to get the latest product news and best practices."},{"question":"What services does AWS offer for generative AI?","answer":"AWS offers a comprehensive set of Generative AI services to help innovate faster."}],"seed_document_id":85,"topic":"AWS Generative AI Services"}}
{"id":"fe037787-bad8-406a-9293-fa0bae1fe6f1","question":"What are some ways to use GPT models and what is their underlying architecture and working principle?","reference_answer":"GPT models can be used to create social media content, convert text to different styles, write and learn code, analyze data, produce learning materials, and build interactive voice assistants. They are neural network-based language prediction models built on the Transformer architecture. They analyze natural language queries, known as prompts, and predict the best possible response based on their understanding of language. They are trained with hundreds of billions of parameters on massive language datasets and can take input context into account to generate long responses.","reference_context":"Document 215: The GPT models are general-purpose language models that can perform a broad range of tasks from creating original content to write code, summarizing text, and extracting data from documents.\nHere are some ways you can use the GPT models:\nCreate social media content\nDigital marketers, assisted by artificial intelligence (AI), can create content for their social media campaigns. For example, marketers can prompt a GPT model to produce an explainer video script. GPT-powered image processing software can create memes, videos, marketing copy, and other content from text instructions.\nConvert text to different styles\nGPT models generate text in casual, humorous, professional, and other styles. The models allow business professionals to rewrite a particular text in a different form. For example, lawyers can use a GPT model to turn legal copies into simple explanatory notes.\u00a0\nWrite and learn code\nAs language models, the GPT models can understand and write computer code in different programming languages. The models can help learners by explaining computer programs to them in everyday language. Also, experienced developers can use GPT tools to autosuggest relevant code snippets.\nAnalyze data\n\nDocument 216: The GPT model can help business analysts efficiently compile large volumes of data. The language models search for the required data and calculate and display the results in a data table or spreadsheet. Some applications can plot the results on a chart or create comprehensive reports.\u00a0\nProduce learning materials\nEducators can use GPT-based software to generate learning materials such as quizzes and tutorials. Similarly, they can use GPT models to evaluate the answers.\nBuild interactive voice assistants\nThe GPT models allow you to build intelligent interactive voice assistants. While many chatbots only respond to basic verbal prompts, the GPT models can produce chatbots with conversational AI capabilities. In addition, these chatbots can converse verbally like humans when paired with other AI technologies.\n\nDocument 214: The GPT models, and in particular, the transformer architecture that they use, represent a significant AI research breakthrough. The rise of GPT models is an inflection point in the widespread adoption of ML because the technology can be used now to automate and improve a wide set of tasks ranging from language translation and document summarization to writing blog posts, building websites, designing visuals, making animations, writing code, ]researching complex topics, and even composing poems. The value of these models lies in their speed and the scale at which they can operate. For example, where you might need several hours to research, write, and edit an article on nuclear physics, a GPT model can produce one in seconds. GPT models have sparked the research in AI towards achieving artificial general intelligence, which means machines can help organizations reach new levels of productivity and reinvent their applications and customer experiences.\n\n\n\n\n\n\n\nWhat are the use cases of GPT?\n\nDocument 218: Though it\u2019s accurate to describe the GPT models as artificial intelligence (AI), this is a broad description. More specifically, the GPT models are neural network-based language prediction models built on the Transformer architecture. They analyze natural language queries, known as prompts, and predict the best possible response based on their understanding of language.\nTo do that, the GPT models rely on the knowledge they gain after they\u2019re trained with hundreds of billions of parameters on massive language datasets. They can take input context into account and dynamically attend to different parts of the input, making them capable of generating long responses, not just the next word in a sequence. For example, when asked to generate a piece of Shakespeare-inspired content, a GPT model does so by remembering and reconstructing new phrases and entire sentences with a similar literary style.","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What are some ways to use GPT models?","answer":"GPT models can be used to create social media content, convert text to different styles, write and learn code, analyze data, produce learning materials, and build interactive voice assistants."},{"question":"What is the underlying architecture of GPT models and how do they work?","answer":"The GPT models are neural network-based language prediction models built on the Transformer architecture. They analyze natural language queries, known as prompts, and predict the best possible response based on their understanding of language. They are trained with hundreds of billions of parameters on massive language datasets and can take input context into account to generate long responses."}],"seed_document_id":215,"topic":"GPT and Neural Networks"}}
{"id":"fce1c42a-df6f-4cef-bb2e-942d7ad158a5","question":"What is Amazon Bedrock and what features does it offer?","reference_answer":"Amazon Bedrock is a fully managed service that makes high-performing foundation models from leading AI startups and Amazon available for your use through a unified API. It offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. You can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.","reference_context":"Document 0: What is Amazon Bedrock? - Amazon BedrockWhat is Amazon Bedrock? - Amazon BedrockAWSDocumentationAmazon BedrockUser GuideFeatures of Amazon BedrockAmazon Bedrock pricingWhat is Amazon Bedrock?Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs)\n  from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI\n  applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily\n  experiment with and evaluate top foundation models for your use cases, privately customize them with your data\n  using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents\n  that execute tasks using your enterprise systems and data sources.With Amazon Bedrock's serverless experience, you can get started quickly, privately customize\n  foundation models with your own data, and easily and securely integrate and deploy them into your\n\nDocument 205: Amazon Bedrock is the easiest way to build and scale generative AI applications with foundation models. Amazon Bedrock is a fully managed service that makes foundation models from Amazon and leading AI startups available through an API, so you can choose from various FMs to find the model that's best suited for your use case.\u00a0With Bedrock, you can speed up developing and deploying scalable, reliable, and secure generative AI applications without managing infrastructure.\nAmazon SageMaker JumpStart, which is a ML hub offering models, algorithms, and solutions, provides access to hundreds of foundation models, including top performing publicly available foundation models. New foundation models continue to be added, including\u00a0Llama 2,\u00a0Falcon, and\u00a0Stable Diffusion XL 1.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next Steps on AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account\n\nDocument 224: Amazon Bedrock\u00a0is the easiest way to build and scale generative AI applications with large language models, also known as foundation models (FMs), similar to GPT-3. Amazon Bedrock gives you access via an API to foundation models from leading AI startups, including AI21 Labs, Anthropic, and Stability AI\u2014along with Amazon\u2019s newest foundation model family, Amazon Titan FMs. With Bedrock\u2019s serverless experience, you can get started quickly, privately customize FMs with your own data, and easily integrate and deploy them into your applications using the AWS tools and capabilities you are familiar with (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage your FMs at scale) without having to manage any infrastructure. Learn more about\u00a0building with foundation models on Amazon Bedrock.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Machine learning next steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nDocument 1: applications using AWS tools without having to manage any infrastructure.TopicsFeatures of Amazon BedrockAmazon Bedrock pricingSupported AWS RegionsKey definitions\nFeatures of Amazon Bedrock\nTake advantage of Amazon Bedrock foundation models to explore the following capabilities. To see feature limitations by Region, see Model support by AWS Region.","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What is Amazon Bedrock?","answer":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API."},{"question":"What are the features of Amazon Bedrock?","answer":"Amazon Bedrock offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. You can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources."}],"seed_document_id":0,"topic":"AI Models and Pricing"}}
{"id":"fdc085bd-55e4-4c53-afa0-cee9407b0a8d","question":"Where can I find the AWS Management Console and what support options does Amazon Web Services offer?","reference_answer":"The AWS Management Console can be found in your profile after signing out of AWS Builder ID and Amazon Web Services offers a Support Center, Expert Help, a Knowledge Center, and an AWS Support Overview.","reference_context":"Document 229: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us\n\nDocument 25: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us\n\nDocument 167: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us\n\nDocument 186: Ti\u1ebfng Vi\u1ec7t\nT\u00fcrk\u00e7e\n\u03a1\u0443\u0441\u0441\u043a\u0438\u0439\n\u0e44\u0e17\u0e22\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\n\u4e2d\u6587 (\u7b80\u4f53)\n\u4e2d\u6587 (\u7e41\u9ad4)\n\n\n\n\n\n Close \n\nMy Profile\nSign out of AWS Builder ID\nAWS Management Console\nAccount Settings\nBilling & Cost Management\nSecurity Credentials\nAWS Personal Health Dashboard\n\n\n\n Close \n\nSupport Center\nExpert Help\nKnowledge Center\nAWS Support Overview\nAWS re:Post\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to return to Amazon Web Services homepage\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n  Get Started for Free \n\n\n  Contact Us","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"Where can I find the AWS Management Console?","answer":"The AWS Management Console can be found in your profile after signing out of AWS Builder ID."},{"question":"What support options does Amazon Web Services offer?","answer":"Amazon Web Services offers a Support Center, Expert Help, a Knowledge Center, and an AWS Support Overview."}],"seed_document_id":229,"topic":"Amazon Web Services"}}
{"id":"9f82eb31-73db-4544-90ee-2f7d0844b4e8","question":"What services does AWS offer for Generative AI and where can I get the latest product news and best practices?","reference_answer":"AWS offers the most comprehensive set of Generative AI services and you can get the latest product news and best practices by reading Generative AI Blogs.","reference_context":"Document 188: Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 169: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 150: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices\n\nDocument 212: Check out Generative AI on AWS \n           \n\n             Innovate faster with the most comprehensive set of Generative AI services \n            \n\n\n\n\n\n\n\n\n\n\n\n             Browse Generative AI Trainings \n           \n\n             Get started on generative AI training with content built by AWS experts \n            \n\n\n\n\n\n\n\n\n\n\n\n             Read Generative AI Blogs \n           \n\n             Get the latest AWS generative AI product news and best practices","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What services does AWS offer for Generative AI?","answer":"AWS offers the most comprehensive set of Generative AI services."},{"question":"Where can I get the latest AWS generative AI product news and best practices?","answer":"You can get the latest AWS generative AI product news and best practices by reading Generative AI Blogs."}],"seed_document_id":188,"topic":"AWS Generative AI Services"}}
{"id":"37dcdf5d-2796-4d96-b4df-97c8e1c3d1bd","question":"What distinguishes conversational AI from generative AI and what are some applications of generative AI?","reference_answer":"Conversational AI focuses on understanding human speech and conversational flow, while generative AI aims to create new and original content from existing data. Applications of generative AI include improving customer experience with chatbots, virtual assistants, intelligent contact centers, personalization, and content moderation; boosting employee productivity with conversational search, content creation, and text summarization; enhancing business operations with intelligent document processing, maintenance assistants, quality control, visual inspection, and synthetic training data generation; and producing creative content such as art, music, text, animation, video, and image generation.","reference_context":"Document 135: Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. Like all artificial intelligence, generative AI is powered by ML models. In particular, they use very large models that are pretrained on vast amounts of data and commonly referred to as foundation models (FMs).\nApart from content creation, you can use generative AI to improve digital image quality, edit videos, build manufacturing prototypes, and augment data with synthetic datasets.\nRead about generative AI \u00bb\nRead about foundation models \u00bb\nConversational AI vs. generative AI\nConversational AI and generative AI have different end goals. The goal of conversational AI is to understand human speech and conversational flow. You can configure it to respond appropriately to different query types and not answer questions out of scope.\nIn contrast, generative AI aims to create new and original content by learning from existing customer data. In one sense, it will only answer out-of-scope questions in new and original ways. Its response quality may not be what you expect, and it may not understand customer intent like conversational AI.\n\nDocument 86: Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation. Generative AI is the next step in artificial intelligence. You can train it to learn human language, programming languages, art, chemistry, biology, or any complex subject matter. It reuses training data to solve new problems. For example, it can learn English vocabulary and create a poem from the words it processes. Your organization can use generative AI for various purposes, like chatbots, media creation, and product development and design.\n\n\n\n\n\n\n\nWhy is generative AI important?\n\nDocument 46: Generative AI refers to artificial intelligence systems that can create new content and artifacts such as images, videos, text, and audio from simple text prompts. Unlike past AI, which was limited to analyzing data, generative AI leverages deep learning and massive datasets to produce high-quality, human-like creative outputs. While enabling exciting creative applications, concerns around bias, harmful content, and intellectual property exist. Overall, generative AI represents a major evolution in AI capabilities to generate human language and new content and artifacts in a human-like manner.\nRead about generative AI \u00bb\n\nDocument 113: Besides code generation, there are many applications where you can put generative AI to work to achieve a step change in customer experience, employee productivity, business efficiency, and creativity. You can use generative AI to improve customer experience through capabilities such as chatbots, virtual assistants, intelligent contact centers, personalization, and content moderation. You can boost your employees\u2019 productivity with generative AI-powered conversational search, content creation, and text summarization, among others. You can improve business operations with intelligent document processing, maintenance assistants, quality control and visual inspection, and synthetic training data generation. Finally, you can use generative AI to turbocharge the production of all types of creative content, from art and music to text, animation, video, and image generation.","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What is the difference between conversational AI and generative AI?","answer":"The goal of conversational AI is to understand human speech and conversational flow, whereas generative AI aims to create new and original content by learning from existing data."},{"question":"What are some applications of generative AI?","answer":"Generative AI can be used to improve customer experience through capabilities such as chatbots, virtual assistants, intelligent contact centers, personalization, and content moderation. It can also boost employees\u2019 productivity with conversational search, content creation, and text summarization. Moreover, it can improve business operations with intelligent document processing, maintenance assistants, quality control and visual inspection, and synthetic training data generation. It can also be used to produce creative content, from art and music to text, animation, video, and image generation."}],"seed_document_id":135,"topic":"Artificial Intelligence Applications"}}
{"id":"acc4693e-3be7-4757-9277-e0e270427cc7","question":"Why is foundation modeling important and what tasks can these models perform?","reference_answer":"Foundation models are poised to significantly change the machine learning lifecycle. It\u2019s faster and cheaper for data scientists to use pre-trained FMs to develop new ML applications rather than train unique ML models from the ground up. They can be used for automating tasks and processes, especially those that require reasoning capabilities. Tasks that FMs can perform include language processing, visual comprehension, code generation, and human-centered engagement. They can answer natural language questions, translate languages, identify images and physical objects, generate computer code based on natural language inputs, and support human decision-making.","reference_context":"Document 192: Why is foundation modeling important?\n\nFoundation models are poised to significantly change the machine learning lifecycle. Although it currently costs millions of dollars to develop a foundation model from scratch, they\u2019re useful in the long run. It\u2019s faster and cheaper for data scientists to use pre-trained FMs to develop new ML applications rather than train unique ML models from the ground up.\nOne potential use is automating tasks and processes, especially those that require reasoning capabilities. Here are a few applications for foundation models:\n\nCustomer support\nLanguage translation\nContent generation\nCopywriting\nImage classification\nHigh-resolution image creation and editing\nDocument extraction\nRobotics\nHealthcare\nAutonomous vehicles\n\n\n\n\n\n\n\n\nHow do foundation models work?\n\nDocument 189: What is a Foundation Model?\nWhat is unique about foundation models?\nWhy is foundation modeling important?\nHow do foundation models work?\nWhat can foundation models do?\nWhat are examples of foundation models?\nWhat are challenges with foundation models?\nHow AWS Can Help?\n\n\n\n\n\n\n\nWhat is a Foundation Model?\n\nTrained on massive datasets, foundation models (FMs) are large deep learning neural networks that have changed the way data scientists approach machine learning (ML). Rather than develop artificial intelligence (AI) from scratch, data scientists use a foundation model as a starting point to develop ML models that power new applications more quickly and cost-effectively. The term foundation model was coined by researchers to describe ML models trained on a broad spectrum of generalized and unlabeled data and capable of performing a wide variety of general tasks such as understanding language, generating text and images, and conversing in natural language.\n\n\n\n\n\n\n\nWhat is unique about foundation models?\n\nDocument 190: A unique feature of foundation models is their adaptability. These models can perform a wide range of disparate tasks with a high degree of accuracy based on input prompts. Some tasks include natural language processing (NLP), question answering, and image classification. The size and general-purpose nature of FMs make them different from traditional ML models, which typically perform specific tasks, like analyzing text for sentiment, classifying images, and forecasting trends.\nYou can use foundation models as base models for developing more specialized downstream applications. These models are the culmination of more than a decade of work that saw them increase in size and complexity.\n\nDocument 195: Foundation models, even though are pre-trained, can continue to learn from data inputs or prompts during inference. This means that you can develop comprehensive outputs through carefully curated prompts. Tasks that FMs can perform include language processing, visual comprehension, code generation, and human-centered engagement.\nLanguage processing\nThese models have remarkable capabilities to answer natural language questions and even the ability to write short scripts or articles in response to prompts. They can also translate languages using NLP technologies.\nVisual comprehension\nFMs excel in computer vision, especially with regard to identifying images and physical objects. These capabilities may find use in applications such as autonomous driving and robotics. Another capability is the generation of images from input text, as well as photo and video editing.\nCode generation\nFoundation models can generate computer code in various programming languages based on natural language inputs. It\u2019s also feasible to use FMs to evaluate and debug code.\nHuman-centered engagement\nGenerative AI models use human inputs to learn and improve predictions. An important and sometimes overlooked application is the ability of these models to support human decision-making. Potential uses include clinical diagnoses, decision support systems, and analytics.","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"Why is foundation modeling important?","answer":"Foundation models are poised to significantly change the machine learning lifecycle. It\u2019s faster and cheaper for data scientists to use pre-trained FMs to develop new ML applications rather than train unique ML models from the ground up. They can be used for automating tasks and processes, especially those that require reasoning capabilities."},{"question":"What tasks can foundation models perform?","answer":"Tasks that FMs can perform include language processing, visual comprehension, code generation, and human-centered engagement. They can answer natural language questions, translate languages, identify images and physical objects, generate computer code based on natural language inputs, and support human decision-making."}],"seed_document_id":192,"topic":"Large Language Models in Machine Learning"}}
{"id":"356133c2-85e3-4d94-870a-1696d32b7152","question":"What is the AWS Free Tier and what resources are available for developers on AWS?","reference_answer":"The AWS Free Tier provides instant access to a free account and for developers on AWS, resources include the Developer Center, SDKs & Tools, and specific language supports like .NET, Python, Java, PHP, and JavaScript on AWS.","reference_context":"Document 140: Next Steps on AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\nDocument 79: Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nDocument 182: Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nDocument 206: Instant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help","conversation_history":[],"metadata":{"question_type":"double","original_questions":[{"question":"What is the AWS Free Tier?","answer":"The AWS Free Tier provides instant access to a free account."},{"question":"What resources are available for developers on AWS?","answer":"For developers on AWS, resources include the Developer Center, SDKs & Tools, and specific language supports like .NET, Python, Java, PHP, and JavaScript on AWS."}],"seed_document_id":140,"topic":"AWS and Artificial Intelligence"}}
{"id":"1cee467f-634c-4e37-9eb7-f21178b5b323","question":"Could you explain what it is?","reference_answer":"A prompt engineer identifies scripts and templates that users can customize and complete to get the best result from the language models. They experiment with different types of inputs to build a prompt library that application developers can reuse in different scenarios. For example, in AI chatbots, a user may enter an incomplete problem statement. The application's code uses an engineered prompt to guide the AI model to generate more relevant and accurate information.","reference_context":"Document 70: Prompt engineering is a dynamic and evolving field. It requires both linguistic skills and creative expression to fine-tune prompts and obtain the desired response from the generative AI tools.\nRead the guide to prompt engineering by AWS PartyRock \u00bb\nHere are some more examples of techniques that prompt engineers use to improve their AI models' natural language processing (NLP) tasks.\nChain-of-thought prompting\nChain-of-thought prompting is a technique that breaks down a complex question into smaller, logical parts that mimic a train of thought. This helps the model solve problems in a series of intermediate steps rather than directly answering the question. This enhances its reasoning ability.\nYou can perform several chain-of-though rollouts for complex tasks and choose the most commonly reached conclusion. If the rollouts disagree significantly, a person can be consulted to correct the chain of thought.\nFor example, if the question is \"What is the capital of France?\" the model might perform several rollouts leading to answers like \"Paris,\" \"The capital of France is Paris,\" and \"Paris is the capital of France.\" Since all rollouts lead to the same conclusion, \"Paris\" would be selected as the final answer.\n\nDocument 67: Prompt engineering techniques are used in sophisticated AI systems to improve user experience with the learning language model. Here are some examples.\nSubject matter expertise\nPrompt engineering plays a key role in applications that require the AI to respond with subject matter expertise. A prompt engineer with experience in the field can guide the AI to reference the correct sources and frame the answer appropriately based on the question asked.\nFor example, in the medical field, a physician could use a prompt-engineered language model to generate differential diagnoses for a complex case. The medical professional only needs to enter the symptoms and patient details. The application uses engineered prompts to guide the AI first to list possible diseases associated with the entered symptoms. Then it narrows down the list based on additional patient information.\nCritical thinking\nCritical thinking applications require the language model to solve complex problems. To do so, the model analyzes information from different angles, evaluates its credibility, and makes reasoned decisions. Prompt engineering enhances a model's data analysis capabilities.\nFor instance, in decision-making scenarios, you could prompt a model to list all possible options, evaluate each option, and recommend the best solution.\nCreativity\n\nDocument 59: Prompt engineering is the process where you guide generative artificial intelligence (generative AI) solutions to generate desired outputs. Even though generative AI attempts to mimic humans, it requires detailed instructions to create high-quality and relevant output. In prompt engineering, you choose the most appropriate formats, phrases, words, and symbols that guide the AI to interact with your users more meaningfully. Prompt engineers use creativity plus trial and error to create a collection of input texts, so an application's generative AI works as expected.\n\n\n\n\n\n\n\nWhat is a prompt?\n\nDocument 63: Prompt engineering jobs have increased significantly since the launch of generative AI. Prompt engineers bridge the gap between your end users and the large language model. They identify scripts and templates that your users can customize and complete to get the best result from the language models. These engineers experiment with different types of inputs to build a prompt library that application developers can reuse in different scenarios.\nPrompt engineering makes AI applications more efficient and effective. Application developers typically encapsulate open-ended user input inside a prompt before passing it to the AI model.\nFor example, consider AI chatbots. A user may enter an incomplete problem statement like, \"Where to purchase a shirt.\" Internally, the application's code uses an engineered prompt that says, \"You are a sales assistant for a clothing company. A user, based in Alabama, United States, is asking you where to purchase a shirt. Respond with the three nearest store locations that currently stock a shirt.\" The chatbot then generates more relevant and accurate information.\nNext, we discuss some benefits of prompt engineering.\nGreater developer control","conversation_history":[{"role":"user","content":"I'm curious about the role of a prompt engineer in AI chatbots."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":70,"topic":"AI Prompt Engineering"}}
{"id":"570d6ca9-4309-4e69-930c-ff95d8d88a1b","question":"Could you explain what their function is?","reference_answer":"Position encoders allow GPT models to prevent ambiguous meanings when a word is used in other parts of a sentence. For example, position encoding allows the transformer model to differentiate the semantic differences between sentences.","reference_context":"Document 101: Transformer-based models\nThe transformer-based generative AI model builds upon the encoder and decoder concepts of VAEs. Transformer-based models add more layers to the encoder to improve performance on text-based tasks like comprehension, translation, and creative writing.\nTransformer-based models use a self-attention mechanism. They weigh the importance of different parts of an input sequence when processing each element in the sequence.\nAnother key feature is that these AI models implement contextual embeddings. The encoding of a sequence element depends not only on the element itself but also on its context within the sequence.\nHow transformer-based models work\nTo understand how transformer-based models work, imagine a sentence as a sequence of words.\nSelf-attention helps the model focus on the relevant words as it processes each word. The transformer-based generative model employs multiple encoder layers called attention heads to capture different types of relationships between words. Each head learns to attend to different parts of the input sequence, allowing the model to simultaneously consider various aspects of the data.\nEach layer also refines the contextual embeddings, making them more informative and capturing everything from grammar syntax to complex semantic meanings.\n\nDocument 219: There are different types of neural networks, like recurrent and convolutional. The GPT models are transformer neural networks. The transformer neural network architecture uses self-attention mechanisms to focus on different parts of the input text during each processing step. A transformer model captures more context and improves performance on natural language processing (NLP) tasks. It has two main modules, which we explain next.\nRead about neural networks \u00bb\nRead about natural language processing (NLP) \u00bb\nEncoder\u00a0\nTransformers pre-process text inputs as embeddings, which are mathematical representations of a word. When encoded in vector space, words that are closer together are expected to be closer in meaning. These embeddings are processed through an encoder component that captures contextual information \u00a0from an input sequence. When it receives input, the transformer network\u2019s encoder block separates words into embeddings and assigns weight to each. Weights are parameters to indicate the relevance of words in a sentence.\nAdditionally, position encoders allow GPT models to prevent ambiguous meanings when a word is used in other parts of a sentence. For example, position encoding allows the transformer model to differentiate the semantic differences between these sentences:\n\nDocument 103: Primitive generative models have been used for decades in statistics to aid in numerical data analysis. Neural networks and deep learning were recent precursors for modern generative AI. Variational autoencoders, developed in 2013, were the first deep generative models that could generate realistic images and speech.\nVAEs introduced the capability to create novel variations of multiple data types. This led to the rapid emergence of other generative AI models like generative adversarial networks and diffusion models. These innovations were focused on generating data that increasingly resembled real data despite being artificially created.\nIn 2017, a further shift in AI research occurred with the introduction of transformers. Transformers seamlessly integrated the encoder-and-decoder architecture with an attention mechanism. They streamlined the training process of language models with exceptional efficiency and versatility. Notable models like GPT emerged as foundational models capable of pretraining on extensive corpora of raw text and fine-tuning for diverse tasks.\nTransformers changed what was possible for natural language processing. They empowered generative capabilities for tasks ranging from translation and summarization to answering questions.","conversation_history":[{"role":"user","content":"I'm interested in understanding the role of position encoders in GPT models."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":101,"topic":"GPT and Neural Networks"}}
{"id":"f53830d1-8677-4586-9197-d79d3ff7f464","question":"What are some of the topics you can explore?","reference_answer":"You can learn about AWS, Cloud Computing, AWS Accessibility, AWS Inclusion, Diversity & Equity, DevOps, Containers, Data Lakes, Artificial Intelligence (AI), Generative AI, Machine Learning (ML), and AWS Cloud Security.","reference_context":"Document 79: Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nDocument 182: Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nDocument 140: Next Steps on AWS\n\n\n\n\n\n\n\n\n           Check out additional product-related resources \n         \n Innovate faster with AWS generative AI services\u00a0\n\n\n\n\n\n\n\n\n           Sign up for a free account \n         \n\nInstant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\nDocument 206: Instant get access to the AWS Free Tier.\n\n Sign up\u00a0\n\n\n\n\n\n\n\n\n           Start building in the console \n         \n\nGet started building in the AWS management console.\n\n Sign in\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n  Sign In to the Console \n\n Learn About AWS\n\nWhat Is AWS?\nWhat Is Cloud Computing?\nAWS Accessibility\nAWS Inclusion, Diversity & Equity\nWhat Is DevOps?\nWhat Is a Container?\nWhat Is a Data Lake?\nWhat is Artificial Intelligence (AI)?\nWhat is Generative AI?\nWhat is Machine Learning (ML)?\nAWS Cloud Security\nWhat's New\nBlogs\nPress Releases\n\n\n\n Resources for AWS\n\nGetting Started\nTraining and Certification\nAWS Solutions Library\nArchitecture Center\nProduct and Technical FAQs\nAnalyst Reports\nAWS Partners\n\n\n\n Developers on AWS\n\nDeveloper Center\nSDKs & Tools\n.NET on AWS\nPython on AWS\nJava on AWS\nPHP on AWS\nJavaScript on AWS\n\n\n\n Help","conversation_history":[{"role":"user","content":"You're interested in learning about the AWS console."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":79,"topic":"AWS and Artificial Intelligence"}}
{"id":"3b7d1fc1-bfd1-4602-bf16-724b11a68cd8","question":"Can you tell me what the benefits are?","reference_answer":"Effective prompts provide intent and establish context to the large language models. They help the AI refine the output and present it concisely in the required format. They also prevent users from misusing the AI or requesting something the AI does not know or cannot handle accurately. Improved user experience is another benefit, as users avoid trial and error and still receive coherent, accurate, and relevant responses from AI tools. Further, it enhances the user-AI interaction so the AI understands the user's intention even with minimal input. Finally, it increases flexibility in AI responses.","reference_context":"Document 76: Good prompt engineering requires you to communicate instructions with context, scope, and expected response. Next, we share some best practices.\nUnambiguous prompts\nClearly define the desired response in your prompt to avoid misinterpretation by the AI. For instance, if you are asking for a novel summary, clearly state that you are looking for a summary, not a detailed analysis. This helps the AI to focus only on your request and provide a response that aligns with your objective.\nAdequate context within the prompt\nProvide adequate context within the prompt and include output requirements in your prompt input, confining it to a specific format. For instance, say you want a list of the most popular movies of the 1990s in a table. To get the exact result, you should explicitly state how many movies you want to be listed and ask for table formatting.\nBalance between targeted information and desired output\n\nDocument 77: Balance simplicity and complexity in your prompt to avoid vague, unrelated, or unexpected answers. A prompt that is too simple may lack context, while a prompt that is too complex may confuse the AI. This is especially important for complex topics or domain-specific language, which may be less familiar to the AI. Instead, use simple language and reduce the prompt size to make your question more understandable.\nExperiment and refine the prompt\nPrompt engineering is an iterative process. It's essential to experiment with different ideas and test the AI prompts to see the results. You may need multiple tries to optimize for accuracy and relevance. Continuous testing and iteration reduce the prompt size and help the model generate better output. There are no fixed rules for how the AI outputs information, so flexibility and adaptability are essential.\n\nDocument 64: Prompt engineering gives developers more control over users' interactions with the AI. Effective prompts provide intent and establish context to the large language models. They help the AI refine the output and present it concisely in the required format.\nThey also prevent your users from misusing the AI or requesting something the AI does not know or cannot handle accurately. For instance, you may want to limit your users from generating inappropriate content in a business AI application.\nImproved user experience\nUsers avoid trial and error and still receive coherent, accurate, and relevant responses from AI tools. Prompt engineering makes it easy for users to obtain relevant results in the first prompt. It helps mitigate bias that may be present from existing human bias in the large language models\u2019 training data.\nFurther, it enhances the user-AI interaction so the AI understands the user's intention even with minimal input. For example, requests to summarize a legal document and a news article get different results adjusted for style and tone. This is true even if both users just tell the application, \"Summarize this document.\"\nIncreased flexibility","conversation_history":[{"role":"user","content":"Let's talk about effective prompt engineering."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":76,"topic":"AI Prompt Engineering"}}
{"id":"a30b4116-6d84-4e36-88ac-fd3bb7efc638","question":"Can you tell me about some of the challenges associated with them?","reference_answer":"Some of the challenges with foundation models include: Infrastructure requirements, as building a foundation model from scratch is expensive and requires enormous resources, and training may take months. Front-end development, as developers need to integrate foundation models into a software stack, including tools for prompt engineering, fine-tuning, and pipeline engineering. Lack of comprehension, as they can provide grammatically and factually correct answers, but have difficulty comprehending the context of a prompt. They also aren\u2019t socially or psychologically aware. Unreliable answers, as answers to questions on certain subject matter may be unreliable and sometimes inappropriate, toxic, or incorrect. Bias, as models can pick up hate speech and inappropriate undertones from training datasets.","reference_context":"Document 204: What are challenges with foundation models?\n\nFoundation models can coherently respond to prompts on subjects they haven\u2019t been explicitly trained on. But they have certain weaknesses. Here are some of the challenges facing foundation models:\n\nInfrastructure requirements. Building a foundation model from scratch is expensive and requires enormous resources, and training may take months.\nFront-end development. For practical applications, developers need to integrate foundation models into a software stack, including tools for prompt engineering, fine-tuning, and pipeline engineering.\nLack of comprehension. Although they can provide grammatically and factually correct answers, foundation models have difficulty comprehending the context of a prompt. And they aren\u2019t socially or psychologically aware.\nUnreliable answers. Answers to questions on certain subject matter may be unreliable and sometimes inappropriate, toxic, or incorrect.\nBias. Bias is a distinct possibility as models can pick up hate speech and inappropriate undertones from training datasets. To avoid this, developers should carefully filter training data and encode specific norms into their models.\n\n\n\n\n\n\n\n\nHow AWS Can Help?","conversation_history":[{"role":"user","content":"I'm interested in discussing foundation models."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":204,"topic":"Large Language Models in Machine Learning"}}
{"id":"61a50df2-7c73-4c1c-b790-cecf3371968e","question":"Can you provide information on this?","reference_answer":"The context does not provide information on how GPT works.","reference_context":"Document 217: How does GPT work?","conversation_history":[{"role":"user","content":"I'm interested in understanding the workings of GPT."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":217,"topic":"GPT and Neural Networks"}}
{"id":"5fea26aa-b37a-4d85-9f72-282516188d41","question":"Could you tell me what this is?","reference_answer":"The variational autoencoder in Stable Diffusion consists of a separate encoder and decoder. The encoder compresses the 512x512 pixel image into a smaller 64x64 model in latent space that's easier to manipulate. The decoder restores the model from latent space into a full-size 512x512 pixel image.","reference_context":"Document 153: As a diffusion model, Stable Diffusion differs from many other image generation models. In principle, diffusion models use Gaussian noise to encode an image. Then, they use a noise predictor together with a reverse diffusion process to recreate the image.\nApart from having the technical differences of a diffusion model, Stable Diffusion is unique in that it doesn\u2019t use the pixel space of the image. Instead, it uses a reduced-definition latent space.\u00a0\u00a0\nThe reason for this is that a color image with 512x512 resolution has 786,432 possible values. By comparison, Stable Diffusion uses a compressed image that is 48 times smaller at 16,384 values. This significantly reduces processing requirements. And it\u2019s why you can use Stable Diffusion on a desktop with an NVIDIA GPU with 8 GB of RAM. The smaller latent space works because natural images aren't random. Stable Diffusion uses variational autoencoder (VAE) files in the decoder to paint fine details like eyes.\n\nDocument 156: The main architectural components of Stable Diffusion include a variational autoencoder, forward and reverse diffusion, a noise predictor, and text conditioning.\nVariational autoencoder\nThe variational autoencoder consists of a separate encoder and decoder. The encoder compresses the 512x512 pixel image into a smaller 64x64 model in latent space that's easier to manipulate. The decoder restores the model from latent space into a full-size 512x512 pixel image.\nForward diffusion\nForward diffusion progressively adds Gaussian noise to an image until all that remains is random noise. It\u2019s not possible to identify what the image was from the final noisy image. During training, all images go through this process. Forward diffusion is not further used except when performing an image-to-image conversion.\nReverse diffusion\n\nDocument 152: Stable Diffusion is important because it\u2019s accessible and easy to use. It can run on consumer-grade graphics cards. For the first time, anyone can download the model and generate their images. You also have control over key hyperparameters, such as the number of denoising steps and the degree of noise applied.\u00a0\nStable Diffusion is user-friendly, and you don't need additional information to create images. It has an active community, so Stable Diffusion has ample documentation and how-to tutorials. The software release is under the Creative ML OpenRAIL-M license, which lets you use, change and redistribute modified software. If you release derivative software, you have to release it under the same license and include a copy of the original Stable Diffusion license.\n\n\n\n\n\n\n\nHow does Stable Diffusion work?","conversation_history":[{"role":"user","content":"I'm interested in the role of the variational autoencoder in Stable Diffusion."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":153,"topic":"Stable Diffusion AI Model"}}
{"id":"f00ace37-d55b-447f-93c1-afb65b296ff5","question":"What types are available for this?","reference_answer":"You can build, deploy, and run generative AI applications on AWS.","reference_context":"Document 211: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat is GPT?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS\n\nDocument 187: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat are Foundation Models?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS \n            \n\n\n\n\n\n\n\n\n\n\n\n             Check out Generative AI on AWS\n\nDocument 149: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat is Stable Diffusion?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS\n\nDocument 168: Products  \n  Solutions  \n  Pricing  \n  Introduction to AWS  \n  Getting Started  \n  Documentation  \n  Training and Certification  \n  Developer Center  \n  Customer Success  \n  Partner Network  \n  AWS Marketplace  \n  Support  \n  AWS re:Post  \n  Log into Console  \n  Download the Mobile App  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Cloud Computing?\nCloud Computing Concepts Hub\nGenerative AI\n\n\nWhat are Large Language Models (LLM)?\n\n\nCreate an AWS Account\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n             Explore Generative AI Services \n           \n\n             Build, deploy, and run generative AI applications on AWS","conversation_history":[{"role":"user","content":"I'm interested in building AI applications on AWS."},{"role":"assistant","content":"How can I help you with that?"}],"metadata":{"question_type":"conversational","seed_document_id":211,"topic":"AWS and Artificial Intelligence"}}
